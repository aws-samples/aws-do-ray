apiVersion: "kubeflow.org/v1"
kind: PyTorchJob
metadata:
  name: pytorch-dt-gpu
spec:
  elasticPolicy:
    rdzvBackend: etcd
    # rdzvHost: 10.100.54.122
    rdzvHost: etcd
    rdzvPort: 2379
    minReplicas: 2
    maxReplicas: 36
    maxRestarts: 100
    metrics:
      - type: Resource
        resource:
          name: cpu
          target:
            type: Utilization
            averageUtilization: 80
  pytorchReplicaSpecs:
    Worker:
      replicas: 3
      restartPolicy: OnFailure
      template:
        metadata:
          labels:
            app: imagenet
        spec:
          nodeSelector:
             node.kubernetes.io/instance-type: "g5.8xlarge"
          affinity:
            podAffinity:
              requiredDuringSchedulingIgnoredDuringExecution:
              - labelSelector:
                  matchExpressions:
                  - key: app
                    operator: In
                    values:
                    - imagenet
                topologyKey: "topology.kubernetes.io/zone"
          containers:
            - name: pytorch
              image: 308278082392.dkr.ecr.us-east-1.amazonaws.com/testing-repo:latest
              imagePullPolicy: Always
              resources:
                requests:
                  nvidia.com/gpu: 1
                  #vpc.amazonaws.com/efa: 32
                  #memory: 80000Mi
                limits:
                  nvidia.com/gpu: 1
                  #vpc.amazonaws.com/efa: 32
                  #memory: 80000Mi
              env:
              - name: LOGLEVEL
                value: "DEBUG"
                #- name: NCCL_SOCKET_INFAME
                #value: "eb"
                #- name: NCCL_SOCKET_INFAME G5 needed this
                #value: "eth"  
                #- name: FI_PROVIDER G5 needed this
                #value: "efa"
                #- name: FI_EFA_USE_DEVICE_RDMA
                #value: "1"
                #- name: FI_EFA_FORK_SAFE
                #value: "1"
                #- name: FI_LOG_LEVEL
                #value: "1"
                #value: "1"
                #- name: FI_EFA_ENABLE_SHM_TRANSFER
                #value: "1"
                #- name: TORCH_DISTRIBUTED_DEBUG
                # value: "DETAIL"
                #- name: TORCH_NCCL_ENABLE_MONITORING
                # value: "1"
                #- name: TORCH_NCCL_TRACE_BUFFER_SIZE
                # value: "20000"
                #- name: TORCH_NCCL_DUMP_ON_TIMEOUT
                # value: "1"
                # - name: TORCH_NCCL_DEBUG_INFO_TEMP_FILE
                #value: "/local/nccl_trace_rank_"
                #- name: NCCL_IB_DISABLE
                #value: "1"
                #- name: NCCL_P2P_DISABLE
                #value: "1" 
              - name: NCCL_SHM_DISABLE
                value: "1"
              - name: NCCL_DEBUG
                value: "INFO"
              #- name: TORCH_DIST_INIT_BARRIER
              #  value: "1"
              #- name: NCCL_IGNORE_DISABLED_P2P
              #  value: "1"
              #- name: NCCL_NVLS_ENABLE
              # value: "0"
              command:
                - bash
                - -c
                - "torchrun --nproc_per_node 1  train_images.py --per_device_train_batch_size 8 --per_device_eval_batch_size 8 --num_workers 6"
                # - "torchrun train_images.py --per_device_train_batch_size 8 --per_device_eval_batch_size 8 --num_workers 6"
              volumeMounts:
                - name: dshm
                  mountPath: /dev/shm
          tolerations:                                       ## TOLERATIONS: These would be the taints set on your node groups
            - key: "ray.io/node-type"                        ## in this case, these taints are set on my worker node group
              operator: "Equal"                              ## if no taints, leave blank or delete "tolerations"
              value: "worker"
              effect: "NoSchedule"
          volumes:
            - name: dshm
              emptyDir:
                medium: Memory