apiVersion: "kubeflow.org/v1"
kind: PyTorchJob
metadata:
  name: pytorch-dt-gpu
spec:
  elasticPolicy:
    rdzvBackend: etcd
    rdzvHost: etcd
    rdzvPort: 2379
    minReplicas: 2
    maxReplicas: 36
    maxRestarts: 100
    metrics:
      - type: Resource
        resource:
          name: cpu
          target:
            type: Utilization
            averageUtilization: 80
  pytorchReplicaSpecs:
    Worker:
      replicas: 3
      restartPolicy: OnFailure
      template:
        metadata:
          labels:
            app: imagenet
        spec:
          # nodeSelector:
          #    node.kubernetes.io/instance-type: "g5.8xlarge"
          affinity:
            podAffinity:
              requiredDuringSchedulingIgnoredDuringExecution:
              - labelSelector:
                  matchExpressions:
                  - key: app
                    operator: In
                    values:
                    - imagenet
                topologyKey: "topology.kubernetes.io/zone"
          containers:
            - name: pytorch
              image: 308278082392.dkr.ecr.us-east-1.amazonaws.com/testing-repo:latest
              imagePullPolicy: IfNotPresent
              resources:
                requests:
                  nvidia.com/gpu: 1
                limits:
                  nvidia.com/gpu: 1
              env:
              - name: LOGLEVEL
                value: "DEBUG"
              - name: NCCL_SHM_DISABLE
                value: "1"
              - name: NCCL_DEBUG
                value: "INFO"
              command:
                - bash
                - -c
                - "torchrun --nproc_per_node 1  train_images.py --per_device_train_batch_size 8 --per_device_eval_batch_size 8 --num_workers 6"
                # - "torchrun train_images.py --per_device_train_batch_size 8 --per_device_eval_batch_size 8 --num_workers 6"
              volumeMounts:
                - name: dshm
                  mountPath: /dev/shm
          tolerations:                                       ## TOLERATIONS: These would be the taints set on your node groups
            - key: "ray.io/node-type"                        ## in this case, these taints are set on my worker node group
              operator: "Equal"                              ## if no taints, leave blank or delete "tolerations"
              value: "worker"
              effect: "NoSchedule"
          volumes:
            - name: dshm
              emptyDir:
                medium: Memory
